{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaq5vDOPA8LbxeVfVSGv5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bV4FJYzCNsob"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(42)\n","\n","# Generate dataset\n","def generate_data(n_samples=1000, x_min=0, x_max=10):\n","    X = np.linspace(x_min, x_max, n_samples).reshape(-1, 1)\n","    y = 3 * X + 2  # Linear function y = 3x + 2\n","\n","    # Store original min/max for denormalization\n","    X_min, X_max = X.min(), X.max()\n","    y_min, y_max = y.min(), y.max()\n","\n","    # Normalize data\n","    X = (X - X_min) / (X_max - X_min)\n","    y = (y - y_min) / (y_max - y_min)\n","\n","    return X, y, X_min, X_max, y_min, y_max\n","\n","# Define activation functions and their derivatives\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return np.where(x > 0, 1, 0)\n","\n","def linear(x):\n","    return x\n","\n","def linear_derivative(x):\n","    return np.ones_like(x)\n","\n","activation_functions = {\n","    'relu': (relu, relu_derivative),\n","    'linear': (linear, linear_derivative)\n","}\n","\n","class NeuralNetwork:\n","    def __init__(self, layers, activations):\n","        self.layers = layers\n","        self.activations = activations\n","        self.weights = []\n","        self.biases = []\n","        self.initialize_weights()\n","\n","    def initialize_weights(self):\n","        for i in range(len(self.layers) - 1):\n","            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(1 / self.layers[i])  # Xavier Initialization\n","            bias = np.zeros((1, self.layers[i + 1]))\n","            self.weights.append(weight)\n","            self.biases.append(bias)\n","\n","    def feedforward(self, X):\n","        self.layer_outputs = [X]\n","        for i in range(len(self.weights)):\n","            activation, _ = activation_functions[self.activations[i]]\n","            X = activation(np.dot(X, self.weights[i]) + self.biases[i])\n","            self.layer_outputs.append(X)\n","        return X\n","\n","    def backpropagation(self, X, y, learning_rate):\n","        output_error = y - self.layer_outputs[-1]\n","        _, derivative = activation_functions[self.activations[-1]]\n","        deltas = [output_error * derivative(self.layer_outputs[-1])]\n","\n","        for i in reversed(range(len(deltas), len(self.weights))):\n","            _, derivative = activation_functions[self.activations[i]]\n","            delta = deltas[-1].dot(self.weights[i].T) * derivative(self.layer_outputs[i])\n","            deltas.append(delta)\n","        deltas.reverse()\n","\n","        for i in range(len(self.weights)):\n","            self.weights[i] += self.layer_outputs[i].T.dot(deltas[i]) * learning_rate\n","            self.biases[i] += np.sum(deltas[i], axis=0, keepdims=True) * learning_rate\n","\n","    def train(self, X, y, epochs, learning_rate):\n","        for epoch in range(epochs):\n","            self.feedforward(X)\n","            self.backpropagation(X, y, learning_rate)\n","            if epoch % 100 == 0 or epoch == epochs - 1:\n","                mse = np.mean(np.square(y - self.layer_outputs[-1]))\n","                print(f'Epoch {epoch + 1}/{epochs}, Error: {mse:.6f}')\n","\n","    def predict(self, X):\n","        return self.feedforward(X)\n","\n","# Generate dataset\n","X, y, X_min, X_max, y_min, y_max = generate_data()\n","\n","# Define network structure\n","input_size = 1\n","output_size = 1\n","hidden_layers = [10, 15, 10]\n","layers = [input_size] + hidden_layers + [output_size]\n","activations = ['relu', 'relu', 'relu', 'linear']\n","\n","# Initialize neural network\n","nn = NeuralNetwork(layers, activations)\n","\n","# Train the neural network\n","nn.train(X, y, epochs=1000, learning_rate=0.00001)\n","\n","# Predict values for all X\n","y_pred = nn.predict(X)\n","\n","# Denormalize predictions\n","y_pred_denorm = y_pred * (y_max - y_min) + y_min\n","\n","# Predict value for x=5\n","x_test = np.array([[5]])\n","x_test_norm = (x_test - X_min) / (X_max - X_min)  # Normalize x_test\n","\n","y_test_pred_norm = nn.predict(x_test_norm)\n","y_test_pred = y_test_pred_norm * (y_max - y_min) + y_min  # Denormalize prediction\n","y_exact = 3 * x_test + 2  # Exact function value\n","print(f'Prediction for x=5: {y_test_pred[0][0]:.6f}, Exact value: {y_exact[0][0]:.6f}')\n","\n","# Plot original data and predictions\n","plt.figure(figsize=(10, 6))\n","plt.plot(X * (X_max - X_min) + X_min, y * (y_max - y_min) + y_min, label='Original Data', color='blue')\n","plt.scatter(X * (X_max - X_min) + X_min, y_pred_denorm, label='Predictions', color='red', s=10)\n","plt.scatter(x_test, y_test_pred, label=f'Prediction for x=5: {y_test_pred[0][0]:.2f}', color='green', s=100, marker='x')\n","plt.scatter(x_test, y_exact, label=f'Exact Value for x=5: {y_exact[0][0]:.2f}', color='orange', s=100, marker='o')\n","plt.title('Original Data vs Predictions')\n","plt.xlabel('X')\n","plt.ylabel('f(X) = 3X + 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]}]}