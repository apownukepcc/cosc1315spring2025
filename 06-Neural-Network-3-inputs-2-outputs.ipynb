{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOomJ+QoXJQ0Oatn0rajXn9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCyAF11uexaV","executionInfo":{"status":"ok","timestamp":1738597749373,"user_tz":420,"elapsed":15030,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"df0aa6cd-e03d-4aad-d2ba-7c08bdc500f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000, Error: 0.206158\n","Epoch 101/1000, Error: 0.006617\n","Epoch 201/1000, Error: 0.004046\n","Epoch 301/1000, Error: 0.003227\n","Epoch 401/1000, Error: 0.002779\n","Epoch 501/1000, Error: 0.002466\n","Epoch 601/1000, Error: 0.002214\n","Epoch 701/1000, Error: 0.002004\n","Epoch 801/1000, Error: 0.001823\n","Epoch 901/1000, Error: 0.001660\n","Epoch 1000/1000, Error: 0.001516\n","Prediction for x_test=[[4 5 6]]: [28.45633997 67.69589597], Exact value: [30 60]\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(42)\n","\n","# Generate dataset\n","def generate_data(n_samples=10000, x_min=0, x_max=10):\n","    X = np.random.uniform(x_min, x_max, (n_samples, 3))\n","    y1 = 2 * X[:, 0] + 3 * X[:, 1] + X[:, 2] + 1\n","    y2 = 3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] ** 2 + 2\n","\n","    y = np.column_stack((y1, y2))\n","\n","    # Normalize inputs and outputs\n","    X_min, X_max = X.min(axis=0), X.max(axis=0)\n","    y_min, y_max = y.min(axis=0), y.max(axis=0)\n","\n","    X = (X - X_min) / (X_max - X_min)\n","    y = (y - y_min) / (y_max - y_min)\n","\n","    return X, y, X_min, X_max, y_min, y_max\n","\n","# Define activation functions and their derivatives\n","def leaky_relu(x, alpha=0.01):\n","    return np.where(x > 0, x, alpha * x)\n","\n","def leaky_relu_derivative(x, alpha=0.01):\n","    return np.where(x > 0, 1, alpha)\n","\n","def linear(x):\n","    return x\n","\n","def linear_derivative(x):\n","    return np.ones_like(x)\n","\n","activation_functions = {\n","    'relu': (leaky_relu, leaky_relu_derivative),\n","    'linear': (linear, linear_derivative)\n","}\n","\n","class NeuralNetwork:\n","    def __init__(self, layers, activations):\n","        self.layers = layers\n","        self.activations = activations\n","        self.weights = []\n","        self.biases = []\n","        self.initialize_weights()\n","\n","    def initialize_weights(self):\n","        for i in range(len(self.layers) - 1):\n","            weight = np.random.randn(self.layers[i], self.layers[i + 1]) * np.sqrt(2 / self.layers[i])  # He Initialization\n","            bias = np.zeros((1, self.layers[i + 1]))\n","            self.weights.append(weight)\n","            self.biases.append(bias)\n","\n","    def feedforward(self, X):\n","        self.layer_outputs = [X]\n","        for i in range(len(self.weights)):\n","            activation, _ = activation_functions[self.activations[i]]\n","            X = activation(np.dot(X, self.weights[i]) + self.biases[i])\n","            self.layer_outputs.append(X)\n","        return X\n","\n","    def backpropagation(self, X, y, learning_rate):\n","        output_error = y - self.layer_outputs[-1]\n","        _, derivative = activation_functions[self.activations[-1]]\n","        deltas = [output_error * derivative(self.layer_outputs[-1])]\n","\n","        for i in reversed(range(len(deltas), len(self.weights))):\n","            _, derivative = activation_functions[self.activations[i]]\n","            delta = deltas[-1].dot(self.weights[i].T) * derivative(self.layer_outputs[i])\n","            deltas.append(delta)\n","        deltas.reverse()\n","\n","        max_grad_norm = 5  # Gradient clipping threshold\n","        for i in range(len(self.weights)):\n","            np.clip(deltas[i], -max_grad_norm, max_grad_norm, out=deltas[i])  # Clip gradients\n","            self.weights[i] += self.layer_outputs[i].T.dot(deltas[i]) * learning_rate\n","            self.biases[i] += np.sum(deltas[i], axis=0, keepdims=True) * learning_rate\n","\n","    def train(self, X, y, epochs, learning_rate):\n","        for epoch in range(epochs):\n","            self.feedforward(X)\n","            self.backpropagation(X, y, learning_rate)\n","            if epoch % 100 == 0 or epoch == epochs - 1:\n","                mse = np.mean(np.square(y - self.layer_outputs[-1]))\n","                print(f'Epoch {epoch + 1}/{epochs}, Error: {mse:.6f}')\n","\n","    def predict(self, X):\n","        return self.feedforward(X)\n","\n","# Generate dataset\n","X, y, X_min, X_max, y_min, y_max = generate_data()\n","\n","# Define network structure\n","input_size = 3\n","output_size = 2\n","hidden_layers = [10, 15, 10]\n","layers = [input_size] + hidden_layers + [output_size]\n","activations = ['relu', 'relu', 'relu', 'linear']\n","\n","# Initialize neural network\n","nn = NeuralNetwork(layers, activations)\n","\n","# Train the neural network\n","nn.train(X, y, epochs=1000, learning_rate=0.00001)\n","\n","# Predict value for new input\n","x_test = np.array([[4, 5, 6]])\n","x_test_norm = (x_test - X_min) / (X_max - X_min)  # Normalize input\n","\n","y_test_pred_norm = nn.predict(x_test_norm)\n","y_test_pred = y_test_pred_norm * (y_max - y_min) + y_min  # Denormalize prediction\n","\n","# Exact function values\n","y_exact = np.array([\n","    2 * x_test[0, 0] + 3 * x_test[0, 1] + x_test[0, 2] + 1,\n","    3 * x_test[0, 0] + 2 * x_test[0, 1] + x_test[0, 2] ** 2 + 2\n","])\n","\n","print(f'Prediction for x_test={x_test}: {y_test_pred[0]}, Exact value: {y_exact}')\n"]}]}