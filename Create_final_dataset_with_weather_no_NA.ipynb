{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVmZ1Vqw+WyQbHf/8lyWXs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GswWfaWXwcUT","executionInfo":{"status":"ok","timestamp":1741202640821,"user_tz":420,"elapsed":21055,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"56eca360-cced-467e-8af5-762efab6bf37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Emissions dataset successfully loaded from GitHub.\n","Emissions Data Columns: ['Plant', 'Source', 'Parameter', 'Units', 'TimeStamp', 'Value', 'Description']\n","Emissions data successfully pivoted.\n","Weather dataset successfully loaded from GitHub.\n","Weather Data Columns: ['date', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres']\n","Weather data successfully merged.\n","Rows with missing values have been deleted.\n","Final dataset with weather data saved to: /content/drive/My Drive/dataset_with_weather.csv\n"]}],"source":["import pandas as pd\n","from google.colab import drive\n","\n","# Mount Google Drive (if not already mounted)\n","drive.mount('/content/drive')\n","\n","# Define file paths/URLs\n","github_emissions_url = \"https://raw.githubusercontent.com/apownukepcc/datathon-spring-2025/main/emissions_data_updated.csv\"\n","github_weather_url   = \"https://raw.githubusercontent.com/apownukepcc/datathon-spring-2025/main/weather.csv\"\n","output_file = \"/content/drive/My Drive/dataset_with_weather.csv\"\n","\n","# Load the emissions dataset\n","try:\n","    emissions_data = pd.read_csv(github_emissions_url)\n","    print(\"Emissions dataset successfully loaded from GitHub.\")\n","except Exception as e:\n","    print(f\"Error loading emissions dataset: {e}\")\n","    exit()\n","\n","# Print columns to verify structure.\n","print(\"Emissions Data Columns:\", emissions_data.columns.tolist())\n","\n","# In this dataset, use 'TimeStamp' as the date column.\n","if \"TimeStamp\" not in emissions_data.columns:\n","    print(\"Error: 'TimeStamp' column not found in emissions data.\")\n","    exit()\n","else:\n","    # Rename TimeStamp to date for consistency.\n","    emissions_data.rename(columns={\"TimeStamp\": \"date\"}, inplace=True)\n","\n","# Ensure that required columns exist in emissions data.\n","required_columns = [\"date\", \"Source\", \"Parameter\", \"Value\"]\n","for col in required_columns:\n","    if col not in emissions_data.columns:\n","        print(f\"Error: Required column '{col}' not found in emissions data.\")\n","        exit()\n","\n","# Pivot the emissions dataset so that each unique 'Parameter' becomes its own column.\n","try:\n","    final_data = emissions_data.pivot_table(index=[\"date\", \"Source\"],\n","                                            columns=\"Parameter\",\n","                                            values=\"Value\").reset_index()\n","    print(\"Emissions data successfully pivoted.\")\n","except Exception as e:\n","    print(f\"Error during pivoting the data: {e}\")\n","    exit()\n","\n","# Define the desired parameters to include as separate columns.\n","desired_params = [\n","    'UNITONBT', 'SO2TONS', 'HEATINBA', 'NH3TONS',\n","    'UNITONBA', 'HEAT_QA', 'HEATINBT', 'GFLOW_BA',\n","    'NOXTONS', 'LOADMWBA', 'COTONS', 'LOADMWBT'\n","]\n","\n","# Ensure each desired parameter is in the pivoted data; if missing, add as empty.\n","for param in desired_params:\n","    if param not in final_data.columns:\n","        final_data[param] = pd.NA\n","\n","# Reorder columns so that 'date' and 'Source' come first.\n","final_data = final_data[['date', 'Source'] + desired_params]\n","\n","# Calculate Emissions_Load for selected emission parameters relative to the load parameter (LOADMWBA).\n","# Here we compute: Emissions_Load = (parameter value) / LOADMWBA for each emission parameter.\n","emission_params = ['SO2TONS', 'NOXTONS', 'COTONS']\n","if 'LOADMWBA' in final_data.columns:\n","    for param in emission_params:\n","        if param in final_data.columns:\n","            # Using .loc to avoid SettingWithCopyWarning\n","            final_data.loc[:, f'Emissions_Load_{param}'] = final_data[param] / final_data['LOADMWBA']\n","else:\n","    print(\"Warning: 'LOADMWBA' column not found. Skipping emissions load calculations.\")\n","\n","# Load the weather dataset.\n","try:\n","    weather_data = pd.read_csv(github_weather_url)\n","    print(\"Weather dataset successfully loaded from GitHub.\")\n","except Exception as e:\n","    print(f\"Error loading weather dataset: {e}\")\n","    exit()\n","\n","# Print weather dataset columns for verification.\n","print(\"Weather Data Columns:\", weather_data.columns.tolist())\n","\n","# Ensure weather data has the 'date' column. If needed, adjust accordingly.\n","if \"date\" not in weather_data.columns and \"Date\" in weather_data.columns:\n","    weather_data.rename(columns={\"Date\": \"date\"}, inplace=True)\n","if \"date\" not in weather_data.columns:\n","    print(\"Error: 'date' column not found in weather data.\")\n","    exit()\n","\n","# Merge the weather data into the final emissions dataset.\n","# Since the weather data does not have a 'Source' column, we merge on 'date' only.\n","try:\n","    final_data_with_weather = pd.merge(final_data, weather_data, on=\"date\", how=\"left\")\n","    print(\"Weather data successfully merged.\")\n","except Exception as e:\n","    print(f\"Error merging weather data: {e}\")\n","    exit()\n","\n","# Delete rows with missing values\n","final_data_with_weather = final_data_with_weather.dropna()\n","print(\"Rows with missing values have been deleted.\")\n","\n","# Save the final merged dataset to a CSV file in Google Drive.\n","try:\n","    final_data_with_weather.to_csv(output_file, index=False)\n","    print(f\"Final dataset with weather data saved to: {output_file}\")\n","except Exception as e:\n","    print(f\"Error saving final dataset: {e}\")\n"]}]}