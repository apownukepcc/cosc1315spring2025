{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpUqcMyVCPtRQcBtWOsBEp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1C4pNs7v9cnn1ltBqPJcoeiuMSp4OWnEe"},"id":"4bfDLr7YZG3U","executionInfo":{"status":"ok","timestamp":1740592506683,"user_tz":420,"elapsed":70665,"user":{"displayName":"Andrew Pownuk","userId":"03168004100943721436"}},"outputId":"2ef91f51-1e22-4c76-e6eb-9b5124549e9e"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Install necessary packages (uncomment if needed)\n","!pip install catboost xgboost lightgbm\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Import models from scikit-learn\n","from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n","                              ExtraTreesRegressor, AdaBoostRegressor)\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.tree import DecisionTreeRegressor  # CART model\n","from sklearn.linear_model import (LinearRegression, Lasso, ElasticNet, BayesianRidge, Ridge)\n","from sklearn.svm import SVR\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","\n","# Import additional models from external libraries\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","\n","# Mount Google Drive (if running in Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Load dataset from online location\n","data_url = \"https://raw.githubusercontent.com/apownukepcc/ForecastingDailyEmissions/refs/heads/main/SO2TONS_dataset.csv\"\n","data = pd.read_csv(data_url)\n","\n","# Convert the 'date' column to datetime\n","data['date'] = pd.to_datetime(data['date'])\n","\n","# Filter to peak season months (May through August)\n","peak_season_months = [5, 6, 7, 8]\n","data = data[data['date'].dt.month.isin(peak_season_months)]\n","\n","# Define predictors and target variable\n","predictors = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres']\n","target = 'Emissions_Load'\n","\n","# Drop rows with missing values (for predictors and target)\n","data = data.dropna(subset=predictors + [target])\n","\n","# Define a comprehensive set of predictive models\n","models = {\n","    \"Random Forest\": RandomForestRegressor(random_state=42),\n","    \"k-NN\": KNeighborsRegressor(n_neighbors=5),\n","    \"Neural Network\": MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42),\n","    \"Linear Regression\": LinearRegression(),\n","    \"CART\": DecisionTreeRegressor(random_state=42),\n","    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n","    \"SVR\": SVR(kernel='rbf'),\n","    \"Extra Trees\": ExtraTreesRegressor(random_state=42),\n","    \"AdaBoost\": AdaBoostRegressor(random_state=42),\n","    \"Lasso\": Lasso(),\n","    \"ElasticNet\": ElasticNet(random_state=42),\n","    \"XGBoost\": XGBRegressor(random_state=42),\n","    \"LightGBM\": LGBMRegressor(random_state=42),\n","    \"CatBoost\": CatBoostRegressor(random_state=42, verbose=0),\n","    \"Bayesian Ridge\": BayesianRidge(),\n","    \"Ridge\": Ridge(random_state=42)\n","}\n","\n","# Create an empty DataFrame to store predictions for all powerplants\n","all_predictions_table = pd.DataFrame()\n","\n","# For overall average relative error later, we'll store per-model errors in a dictionary\n","overall_rel_errors = {model_name: [] for model_name in models.keys()}\n","\n","# Loop over each unique powerplant in the \"Source\" column\n","for source in data['Source'].unique():\n","    print(f\"Processing predictions for powerplant: {source}\")\n","\n","    # Filter data for the current powerplant\n","    data_source = data[data['Source'] == source].copy()\n","\n","    # Check if there is enough data for meaningful predictions\n","    if data_source.shape[0] < 10:  # Adjust threshold as needed\n","        print(f\"Not enough data for {source}, skipping...\\n\")\n","        continue\n","\n","    # Split features and target\n","    X = data_source[predictors]\n","    y = data_source[target]\n","\n","    # Split into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    # Convert y_test to a NumPy array to ensure proper alignment\n","    y_test_array = y_test.values\n","\n","    # Dictionaries to hold predictions and performance metrics for this powerplant\n","    predictions = {}\n","    performance_metrics = {}\n","\n","    # Train each model and compute predictions and metrics\n","    for model_name, model in models.items():\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        predictions[model_name] = y_pred\n","\n","        # Calculate performance metrics using y_test_array\n","        rmse = np.sqrt(mean_squared_error(y_test_array, y_pred))\n","        mae = mean_absolute_error(y_test_array, y_pred)\n","        r2 = r2_score(y_test_array, y_pred)\n","        mape = np.mean(np.abs((y_test_array - y_pred) / y_test_array)) * 100\n","        performance_metrics[model_name] = {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"MAPE\": mape}\n","\n","        # Collect relative errors for overall summary\n","        rel_error = np.mean((np.abs(y_pred - y_test_array) / y_test_array) * 100)\n","        overall_rel_errors[model_name].append(rel_error)\n","\n","    # Build a predictions table for the current powerplant.\n","    # Use .values to ensure the order is consistent.\n","    pred_table = pd.DataFrame({\n","        \"Date\": data_source.loc[X_test.index, 'date'].values,\n","        \"Actual\": y_test_array\n","    })\n","\n","    # For each model, add prediction, residual, and relative error columns; also plot results\n","    for model_name, y_pred in predictions.items():\n","        pred_table[model_name] = y_pred\n","        residual = y_pred - y_test_array\n","        pred_table[model_name + \" Residual\"] = residual\n","        pred_table[model_name + \" Relative Error (%)\"] = (np.abs(residual) / y_test_array) * 100\n","\n","        # Plot predicted vs. actual\n","        plt.figure(figsize=(8, 6))\n","        plt.scatter(y_test_array, y_pred, alpha=0.6, label=model_name)\n","        plt.plot([min(y_test_array), max(y_test_array)], [min(y_test_array), max(y_test_array)], 'k--', label=\"Perfect Fit\")\n","        plt.xlabel(\"Actual Emissions_Load\")\n","        plt.ylabel(\"Predicted Emissions_Load\")\n","        plt.title(f\"{source} - Predicted vs Actual for {model_name}\")\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","\n","    # Print performance metrics for the current powerplant\n","    print(f\"Performance metrics for powerplant {source}:\")\n","    for model_name, metrics in performance_metrics.items():\n","        print(f\"  {model_name}: RMSE={metrics['RMSE']:.2f}, MAE={metrics['MAE']:.2f}, R2={metrics['R2']:.2f}, MAPE={metrics['MAPE']:.2f}%\")\n","    print(\"\\n\")\n","\n","    # Add the Source column and append to the global predictions table\n","    pred_table[\"Source\"] = source\n","    all_predictions_table = pd.concat([all_predictions_table, pred_table], ignore_index=True)\n","\n","# Sort the global predictions table by Source and Date\n","all_predictions_table.sort_values(by=[\"Source\", \"Date\"], inplace=True)\n","\n","# Save the global predictions table to CSV on Google Drive\n","csv_path = '/content/drive/My Drive/final_predictions_by_powerplant_extended.csv'\n","all_predictions_table.to_csv(csv_path, index=False)\n","print(f\"Global predictions table saved to {csv_path}\")\n","\n","# Compute and print overall average relative error for each model across all powerplants\n","print(\"\\nOverall Average Relative Error (%) for each model:\")\n","for model_name, errors in overall_rel_errors.items():\n","    if errors:\n","        avg_rel_error = np.mean(errors)\n","        print(f\"  {model_name}: {avg_rel_error:.2f}%\")\n","    else:\n","        print(f\"  {model_name}: No data available.\")\n","\n","# Optionally, display a sample of the global predictions table\n","print(\"\\nSample of global predictions table:\")\n","print(all_predictions_table.head())\n"]}]}